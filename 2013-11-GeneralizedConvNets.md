
Generalized Convolutional Neural Networks
==========================================

Generalized Convolutional Neural Networks
-----------------------------------------

```
from:    Christopher Olah <christopherolah.co@gmail.com>
to:      Yoshua Bengio <yoshua.bengio@gmail.com>
date:    Mon, Nov 18, 2013 at 6:36 AM
subject: Generalized Convolutional Neural Networks
```
</br>

Dear Yoshua,

I'd like to try to convey a neural nets idea I am presently very
excited by. I've had mixed results explaining it so far, so please
don't waste too much time if I am doing a poor job -- just tell me
where I lose you and I can try again, or explain it in person in a
week. (I have tried to explain the beginning several times before,
while the later sections are my first attempt at explaining them... So
I expect the earlier sections to be better explained.)

It's also ended up being much longer than I expected when I started
writing... But it feels like it would kind of be a waste to not send
it at this point! Please feel free to skim or ignore. I've got a lot
of value just from writing this out clearly.


**Introduction**

Convolutional neural networks strike me as an extremely powerful tool,
and make me wonder a lot about the connection between symmetry and
neural networks. I've seen a number of people consider questions
relating to this: Can we construct neural networks that are invariant
to certain transformations of the input data? Can we design features
that are invariant to particular transformations?

I don't think this is quite the right way to look at things. What I'm
interested in is symmetries *between useful features*.

Given a feature f(x) that is useful in classifying x, is there some
variation of f'(x) which is also useful in classifying x? We can look
at standard convolutional neural networks as saying, "Yes, if f(x) is
a useful feature, f applied to a translated version of x, f(Tx), is
also a useful feature." And then we use all these translated features.

Whenever we can do this, we win a lot! If we can turn one useful
feature into N different useful features, we get:

(1) A more powerful model without increasing the number of parameters.
(Though, they may increase in the next layer.) (Few parameters
protects us from over-fitting, requires less memory, etc.)
(2) Easier to learn features -- they effectively have N times as much
training data.
(3) Potentially, better generalization.

So, where else can we find opportunities like this?


**Generalized Convolutional Neural Networks Through Groups**

The features one observes in the earlier layers of convolutional
networks for image classification tasks are very striking in the
duplication. There are lots and lots of copies of the same feature,
rotated and scaled different amounts. It calls out for an extension of
convolutional neural networks to exploit greater symmetries.

Formally, we can consider the group generated by translations,
rotations, and scalings on the plane, which we shall call G. Now,
given a useful feature f(x) and g ∈ G, we can get a new feature by
applying f to x acted on by g, f(gx).

One needs to take some care in the next layer. For example, if f(x)
detects a vertical chain of vertical line feature activations, we want
the 90 degree rotated version, f'(x), to detect a horizontal chain of
horizontal line features, not a horizontal line of vertical line
features. Or, when we are detecting at a larger scale, we want to look
at larger features from the previous layer. The relationships between
different components of the input need to not be changed by the
transformation.

This actually works out very elegantly.

Typically in convolutional networks, we think of later layers as being
tensors of features: ℝ^{W*H*N} where W is the width of the image, H
the height, and N the number of distinct features. Each value T[w,h,n]
of the feature tensor, T, is the result of applying the nth feature
detector to the input translated by (w,h).

But you can also think of them as being functions ℤ² → ℝ^N. That is,
the function (w,h) → (T[w,h,0], T[w,h,1]... ). For a certain (w,h)
index (i.e. a certain translation transformation), it gives the
feature vector for that transformed input.

Similarly, if we are considering N distinct features f₁, f₂... for all
actions of G, we get a function G → ℝ^N:

g → (f₁(gx), f₂2(gx)...)

Just as convolutional networks can be regarded as normal fully
connected layers, except with the matrix multiplication replaced with
matrix convolutions, we can regard these more general convolutional
networks as replacing multiplication with *group* convolutions (or,
better yet, group cross-correlations). Then everything just works out.
I'm hand-waving over details here, but this can be relatively easily
seen from a visual interpretation of group cross-correlations based on
Cayley diagrams.

This is much more general than just a better way to use convolutional
networks for image classification. It can be used in lots of different
types of input data, as long as there's some natural group action on
it. An easy example that I'm interested in is classifying 3D
volumetric data, but you could use this in lots of cases where the
standard convolutional approach just doesn't make any sense to apply!
For example, I imagine that lots of scientific data has interesting
symmetries in it to exploit.

(And there isn't really an obvious reason you couldn't apply this sort
of thing to groupoids, monoids, or even categories.)


**Neural Networks Applied to Graph Structures**

Generalizing convolutional neural networks to be able to exploit any
group is nice, of course, but many types of input data we might like
to apply them to do not have any nice group structure. I believe we
can push this much further. In this section, we will explore a
generalization to (preferably acyclic) graph input data.

There are lots of places where it is interesting to classify labeled
acyclic graphs. A natural example is classification of chemicals.

```
H - Cl             -> Acid, Corrosive


    H   H
    |   |
H - C - C - O - H  -> Alcohol, Volatile, Flammable
    |   |
    H   H
```

(Not all chemicals are acyclic, of course, but for now we will only
consider those that are.)

How will we even encode these as input for a neural net? In image
data, there is a clear consistent way to map pixels to input neurons.
But not so here.

Perhaps we could render chemicals into chemical diagrams and feed them
into the neural net as pictures? Well, we could, but that seems very
silly and inefficient! There must be a better way!

Let's consider the example of ethanol. Just as, in a convolutional
neural network we can think of ourselves as computing a feature vector
centered on every pixel in the input image, here we will compute a
feature centered on each element! Let's focus on the first C.

```
    H   H
    |   |
H -*C*- C - O - H
    |   |
    H   H
```

From it's perspective, the molecule is CHHH(CHH(OH)).

Or C(CHH(OH))HHH. Or C(C(OH)HH)HHH. Or any other permutation of the 12
ordering of the bonds! But we'll set that aside for the moment and
focus on the issue of how to calculate a feature for CHHH(CHH(OH)).

Once we fix the bond ordering, every other element in the molecule has
a "position" relative to our chosen starting element.

```
    3  4.2
    |   |
2 - * - 4 - 4.3 - 4.3.1
    |   |
    1  4.1
```

In position 1, 2, and 3 are Hydrogen atoms, in position 4 is a Carbon,
in position 4.1 and 4.2 are, again Hydrogens... And so on.

In each position we can have a given element, which we represent as a
basis "element" vector where each dimension represents an element.
That is:

```
H  = (1, 0, 0... )
He = (0, 1, 0... )
...
```

If there is no element in that position (e.g. there's no 2.1 element
in the above example), the element vector is null. By stacking the
element vectors for each position, we get a matrix with positions as
one axis and elements as another. (Just as, in regular convolutional
neural nets we only look at a patch of features a few pixels wide,
here we will likely only want to look at positions up to a few steps
away.)

The weights for a feature, then, are a matrix of the same shape: a
weight for every feature and position. To compute the feature, we
flatten the matrices, take their dot product, subtract our bias, and
apply our non-linearity.

Now let's return to the bond permutation issue. What do we do about
the fact that the relative positions can be permuted in all sorts of
ways? One possibility would be to create a canonical way to choose the
ordering of branches (e.g. longest goes first). But I think a much
more natural solution is to try to detect our features for all
permutations and take the maximum: we care if a feature is present
somewhere, not how it is represented.

(This might sound very computationally expensive, but so long as our
non-linearity is monotone, it actually isn't that bad because we can
do things like optimize the permutation of different branches
independently.)

The result of doing this operation centered at each element is a new
graph with the same structure as the molecule, but instead of having
element vectors at each point it has a feature vector. We can repeat
this several times, just as we can have multiple convolutional layers
in a convolutional network.

Finally we select one or several molecule positions and feed their
feature vectors into a fully connected layer and do classification.

(Note that some of the core ideas here could be reversed to allow
unsupervised learning of labeled graph structures,  like classes of
chemicals. You would grow the molecule graph outward from a single
node over several layers, or directly induce probabilities over graph
structures.)

(If you interpret the graphs in a very strange way, you can view this
as a group convolution network under an action of S₄*ℤ*ℤ*...*ℤ or for
the special case of chemicals, just S₄*ℤ*ℤ. Max pooling is done to get
rid of S₄ words.)


**Neural networks Applied to Algebraic Data Types**

Algebraic data types are an extremely powerful way to describe data
structures. A classic example is the Binary Tree, given here with
leaves containing integers:

```haskell
data BinaryTree = Leaf Integer | Branch BinaryTree BinaryTree
```

On the left, we have the name of the data structure we are declaring,
and on the right we have its constructors with its argument types.

For example, if we want to represent this tree:

```
  /\
 /\ 3
 1 2
```

We would get:

```haskell
Branch (Branch (Leaf 1) (Leaf 2)) (Leaf 3)
```

Just as we had relative positions in the chemical structure, we can
have relative positions in our binary tree. Movements are the constructor
argument positions: Leaf argument 1, Branch argument 1,
Branch argument 2. We'll refer to them as L1, B1 and B2 for brevity. For example:

```
 Branch (Branch (Leaf 1) (Leaf 2)) (Leaf 3)
   *    (B1     (B1.B1 ) (B1.B2) ) (B2    )
```

Another example where we give positions of leaf arguments:

```
Branch (Leaf (  1  ) ) (Leaf (  2  ) )
   *   (B1   (B1.L1) ) (B2   (B2.L1) )
```

Every constructor is a position we could detect a feature at,
detecting properties of the tree downward from that point. And so we
do, at each one. Now we have a tree where each node has a feature
vector. We repeat in the natural way.

(This can probably be looked at as some sort of "monoid convolution".)

After a few layers, we could feed the top node into a fully connected
network and do classification.

It may not seem very exciting to do classification of binary trees,
but algebraic data types are a remarkably expressive way to describe
data. For example, we can represent numbers:

```haskell
data Nat = Zero | Succ Nat
```

(For example, two is `Succ (Succ Zero)` -- the successor of the
successor of zero.)

We can also represent symbolic mathematical expressions:

```haskell
data Expr = X | Y | Add Expr Expr | Mult Expr Expr
```

(We would represent `x*(x+y)` as `Mult X (Add X Y)`.)

I think where this really gets interesting, though, is when we stop
working on classification problems.

One really interesting application of convolutional neural networks is
to learn local image transformations. For example, a very simple
example that I've done is cleaning up images of blackboards.

Similarly, Algebraic Data Type convolutional neural nets should be
able to learn local transformations of their structures. Such local
transformations are extremely interesting.

For example, multiplying a number by two can be viewed as such a local
transformation:

```haskell
double  Zero    = Zero
double (Succ x) = Succ (Succ (double x))
```

The neural net could be interpreted as inducing probabilities over
different computation paths. We would train to update weights to
increase the probability of computation paths giving the right
answers. (This would be prohibitively expensive outside of trivial
examples unless one can come up with something very clever.)

Perhaps more exciting is the idea of having a neural network learn
symbolic mathematical operations. (Likely, you would need a recurrent
neural network to make this interesting... which makes this wild
thought even more unrealistic. But perhaps echo-state networks could
make it work? The natural idea would seem to be some sort of
"catamorphic" neural net, analogous to the class of functions called
catamorphisms in functional programming.)

**Conclusion**


The ideas in this email get wilder as it goes on. I have not done any
experiments to support the ideas outlined and they may very well be
crazy. (And I would be very grateful for criticism before I invest
huge amounts of time exploring them.)

Despite my low confidence in these ideas, I am extremely excited about them!

Chris Olah

</br>
</br>
</br>



Re: Generalized Convolutional Neural Networks
----------------------------------------------

```
from:    Yoshua Bengio <yoshua.bengio@gmail.com>
to:      Christopher Olah <christopherolah.co@gmail.com>
date:    Tue, Nov 19, 2013 at 6:01 AM
subject: Re: Generalized Convolutional Neural Networks
```
</br>


> Dear Yoshua,
>
> I'd like to try to convey a neural nets idea I am presently very
> excited by. I've had mixed results explaining it so far, so please
> don't waste too much time if I am doing a poor job -- just tell me
> where I lose you and I can try again, or explain it in person in a
> week. (I have tried to explain the beginning several times before,
> while the later sections are my first attempt at explaining them... So
> I expect the earlier sections to be better explained.)
>
> It's also ended up being much longer than I expected when I started
> writing... But it feels like it would kind of be a waste to not send
> it at this point! Please feel free to skim or ignore. I've got a lot
> of value just from writing this out clearly.

I like your way of putting things. It shows good psychological skills.

>
>
> **Introduction**
>
> Convolutional neural networks strike me as an extremely powerful tool,
> and make me wonder a lot about the connection between symmetry and
> neural networks. I've seen a number of people consider questions
> relating to this: Can we construct neural networks that are invariant
> to certain transformations of the input data? Can we design features
> that are invariant to particular transformations?

Indeed. Although the quest in deep learning is more like “can we design
a LEARNING ALGORITHM that will DISCOVER features that are invariant
(or extremely sensitive to) specific factors in the data, without having to
explicitly tell the computer about these factors?"

>
> I don't think this is quite the right way to look at things. What I'm
> interested in is symmetries *between useful features*.
>
> Given a feature f(x) that is useful in classifying x, is there some
> variation of f'(x) which is also useful in classifying x? We can look
> at standard convolutional neural networks as saying, "Yes, if f(x) is
> a useful feature, f applied to a translated version of x, f(Tx), is
> also a useful feature." And then we use all these translated features.
>
> Whenever we can do this, we win a lot! If we can turn one useful
> feature into N different useful features, we get:
>
> (1) A more powerful model without increasing the number of parameters.
> (Though, they may increase in the next layer.) (Few parameters
> protects us from over-fitting, requires less memory, etc.)

Right. Although unless you know T ahead of time, you have to learn it too.

> (2) Easier to learn features -- they effectively have N times as much
> training data.
> (3) Potentially, better generalization.
>
> So, where else can we find opportunities like this?
>
>
> **Generalized Convolutional Neural Networks Through Groups**
>
> The features one observes in the earlier layers of convolutional
> networks for image classification tasks are very striking in the
> duplication. There are lots and lots of copies of the same feature,
> rotated and scaled different amounts. It calls out for an extension of
> convolutional neural networks to exploit greater symmetries.

Yes. This has been observed by many before. It is particularly striking
when you look at the filters obtained from sparse coding on natural images.
Look at this: http://arxiv.org/pdf/1109.6638.pdf

>
> Formally, we can consider the group generated by translations,
> rotations, and scalings on the plane, which we shall call G. Now,
> given a useful feature f(x) and g ∈ G, we can get a new feature by
> applying f to x acted on by g, f(gx).

Indeed. Note one practical problem: if G has many dimensions,
then even when discretizing values of g in G, you get an explosion
of the number of possible values of g to look at (exponential in dim(G)).

A practical answer to this dilemma is explored by Geoff Hinton
with his ‘capsules’ idea, i.e., have detectors which detect when
ONE f(gx) is strongly responding, and outputs both the response
and the associated “pose parameters” (i.e., the value of g).

http://www.cs.toronto.edu/~hinton/absps/transauto6.pdf

>
> One needs to take some care in the next layer. For example, if f(x)
> detects a vertical chain of vertical line feature activations, we want
> the 90 degree rotated version, f'(x), to detect a horizontal chain of
> horizontal line features, not a horizontal line of vertical line
> features.

Yes. But you would like a learner to figure that kind of thing automatically,
because hand-coding this kind of knowledge is going to be tedious and
incomplete.

> Or, when we are detecting at a larger scale, we want to look
> at larger features from the previous layer. The relationships between
> different components of the input need to not be changed by the
> transformation.
>
> This actually works out very elegantly.
>
> Typically in convolutional networks, we think of later layers as being
> tensors of features: ℝ^{W*H*N} where W is the width of the image, H
> the height, and N the number of distinct features. Each value T[w,h,n]
> of the feature tensor, T, is the result of applying the nth feature
> detector to the input translated by (w,h).
>
> But you can also think of them as being functions ℤ² → ℝ^N. That is,
> the function (w,h) -> (T[w,h,0], T[w,h,1]... ). For a certain (w,h)
> index (i.e. a certain translation transformation), it gives the
> feature vector for that transformed input.

Yes.

>
> Similarly, if we are considering N distinct features f₁, f₂... for all
> actions of G, we get a function G → ℝ^N:
>
> g → (f₁(gx), f₂(gx)...)
>
> Just as convolutional networks can be regarded as normal fully
> connected layers, except with the matrix multiplication replaced with
> matrix convolutions, we can regard these more general convolutional
> networks as replacing multiplication with *group* convolutions (or,
> better yet, group cross-correlations). Then everything just works out.
> I'm hand-waving over details here, but this can be relatively easily
> seen from a visual interpretation of group cross-correlations based on
> Cayley diagrams.

I’m sorry. I don’t k now Cayley diagrams. Do you want to teach me?

>
> This is much more general than just a better way to use convolutional
> networks for image classification. It can be used in lots of different
> types of input data, as long as there's some natural group action on
> it. An easy example that I'm interested in is classifying 3D
> volumetric data, but you could use this in lots of cases where the
> standard convolutional approach just doesn't make any sense to apply!
> For example, I imagine that lots of scientific data has interesting
> symmetries in it to exploit.

A lot of people in computer vision have followed that path.
One really interesting path in this direction is the one from Stephane Mallat
with his scattering transforms idea:

 http://www.di.ens.fr/data/scattering/

That seems like a reasonable way to do this. It is reminiscent of
the work on graph kernels (creating a ton of features smartly designed
to provide some kind of desired invariances).

> (Just as, in regular convolutional
> neural nets we only look at a patch of features a few pixels wide,
> here we will likely only want to look at positions up to a few steps
> away.)

Interesting recent work (Montavon & Muller) to apply neural nets to molecules:
(apparently VERY successful)

http://iopscience.iop.org/1367-2630/15/9/095003/pdf/1367-2630_15_9_095003.pdf

>
> The weights for a feature, then, are a matrix of the same shape: a
> weight for every feature and position. To compute the feature, we
> flatten the matrices, take their dot product, subtract our bias, and
> apply our non-linearity.

Isn’t there going to be a very large number of possible features?

>
> Now let's return to the bond permutation issue. What do we do about
> the fact that the relative positions can be permuted in all sorts of
> ways?

See the above Montavon paper. They do a kind of ordering (but that
would be too brittle, alone) train with many local permutations/perturbations.


> One possibility would be to create a canonical way to choose the
> ordering of branches (e.g. longest goes first).

Yes, that is how the Montavon paper starts.

> But I think a much
> more natural solution is to try to detect our features for all
> permutations and take the maximum: we care if a feature is present
> somewhere, not how it is represented.

Don’t you lose too much information?

> **Neural networks Applied to Algebraic Data Types**
>
> Algebraic data types are an extremely powerful way to describe data
> structures.

You might find that paper by Leon Bottou interesting, along this line of thought:

http://leon.bottou.org/news/from_machine_learning_to_machine_reasoning

> A classic example is the Binary Tree, given here with
> leaves containing integers:
>
>```haskell
>data BinaryTree = Leaf Integer | Branch BinaryTree BinaryTree
>```
>
> On the left, we have the name of the data structure we are declaring,
> and on the right we have its constructors with its argument types.
>
> For example, if we want to represent this tree:
>
>```
> /\
> /\ 3
> 1 2
>```

Sorry I did not parse the above 3 lines.

> We would get:
>
>```
>Branch (Branch (Leaf 1) (Leaf 2)) (Leaf 3)
>```
>
> Just as we had relative positions in the chemical structure, we can
> have relative positions in our binary tree. Movements are constructor
> argument positions: Leaf argument 1 (L1), Branch argument 1 (B1),
> Branch argument 2 (B2). For example:
>
>```
>Branch (Branch (Leaf 1) (Leaf 2)) (Leaf 3)
>  *    (B1     (B1.B1 ) (B1.B2) ) (B2    )
>```
>
> Another example where we give positions of leaf arguments:
>
>```
>Branch (Leaf (  1  ) ) (Leaf (  2  ) )
>   *   (B1   (B1.L1) ) (B2   (B2.L1) )
>```
>
> Every constructor is a position we could detect a feature at,
> detecting properties of the tree downward from that point. And so we
> do, at each one. Now we have a tree where each node has a feature
> vector. We repeat in the natural way.
>
> (This can probably be looked at as some sort of "monoid convolution”.)

Another set of related ideas, for natural language processing,
where the Stanford people have made this idea of learning operators
that form trees actually work quite well in practice:

http://www.socher.org/

Sorry I have to go. Will read the rest later.

